import streamlit as st
import os
import requests
import json

# LangChain ë° Gemini ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# --- 1. ì• í”Œë¦¬ì¼€ì´ì…˜ ê¸°ë³¸ ì„¤ì • ë° í”„ë¡¬í”„íŠ¸ ---
st.set_page_config(page_title="í•„í„°ë§ íŠ¹í—ˆ ë¶„ì„ Q&A", layout="wide")

prompt_template = """
You are a helpful AI assistant specializing in patent analysis.
Based on the following retrieved patent documents, answer the user's question.
If the documents don't provide enough information, say that you cannot find a relevant answer in the provided documents.
Provide a clear and concise answer, and always cite the source patent documents you used by their filenames (e.g., `[us20230012345a1p.txt]`).

**Retrieved Documents:**
{context}

**User's Question:**
{question}

**Your Answer:**
"""
PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

# --- 2. ì‚¬ì´ë“œë°” - ì„¤ì • ---
with st.sidebar:
    st.header("âœ¨ AI & DB ì„œë²„ ì„¤ì •")
    gemini_api_key = st.text_input("Gemini API Key", type="password", help="[Google AI Studio](https://aistudio.google.com/app/apikey)ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”.")
    db_server_url = st.text_input("DB ê²€ìƒ‰ ì„œë²„ ì£¼ì†Œ", help="ê¸°ìˆ™ì‚¬ PCì˜ Tailscale IP ì£¼ì†Œ ë˜ëŠ” localhostë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    st.markdown("---")
    st.header("ğŸ¢ ì „ë¬¸ê°€ ì„ íƒ")
    # [ìˆ˜ì •] ê°•í™”ëœ DBë¥¼ ì‚¬ìš©í•˜ë„ë¡ company_id ë³€ê²½
    company_options = {
        "ì‚¼ì„±ì „ì (ê°•í™”DB)": "samsung_enriched", 
        "SKí•˜ì´ë‹‰ìŠ¤ (ê°•í™”DB)": "hynix_enriched"
    }
    selected_company_name = st.selectbox("ëŒ€í™”í•  ì „ë¬¸ê°€ë¥¼ ì„ íƒí•˜ì„¸ìš”.", options=company_options.keys())
    selected_company_id = company_options[selected_company_name]
    
    # [í•µì‹¬] ì‚¬ìš©ìê°€ í•„í„°ë¥¼ ì„ íƒí•  ìˆ˜ ìˆëŠ” UI ì¶”ê°€
    st.markdown("---")
    st.header("ğŸ” ê²€ìƒ‰ í•„í„°")
    judgment_filter = st.radio(
        "íŒë‹¨ ê²°ê³¼ë¡œ í•„í„°ë§:",
        ("ì „ì²´", "ì í•©ë§Œ ë³´ê¸°"),
        index=0, # ê¸°ë³¸ê°’ì€ 'ì „ì²´'
        horizontal=True
    )
    # ì„ íƒëœ ê°’ì— ë”°ë¼ APIì— ë³´ë‚¼ ê°’ ì„¤ì •
    judgment_to_send = "Suitable" if judgment_filter == "ì í•©ë§Œ ë³´ê¸°" else "All"
    
    if st.button("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

# --- 3. ë©”ì¸ Q&A ë¡œì§ ---
st.title(f"â˜ï¸ {selected_company_name} íŠ¹í—ˆ ë¶„ì„ Q&A")

if not gemini_api_key or not db_server_url:
    st.info("ì‚¬ì´ë“œë°”ì— Gemini API Keyì™€ DB ê²€ìƒ‰ ì„œë²„ ì£¼ì†Œë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
else:
    # [ìˆ˜ì •] í•„í„°ê°€ ë³€ê²½ë˜ë©´ ì±„íŒ… ê¸°ë¡ì„ ì´ˆê¸°í™”í•˜ë„ë¡ ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
    if "messages" not in st.session_state or st.session_state.get("current_filter") != judgment_to_send or st.session_state.get("current_company") != selected_company_id:
        st.session_state.messages = []
        st.session_state.current_filter = judgment_to_send
        st.session_state.current_company = selected_company_id

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input(f"[{judgment_filter}] {selected_company_name} íŠ¹í—ˆì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("DB ì„œë²„ì— í•„í„°ë§ ê²€ìƒ‰ ìš”ì²­ ì¤‘..."):
                try:
                    # [ìˆ˜ì •] DB ê²€ìƒ‰ API í˜¸ì¶œ ì‹œ, ì„ íƒëœ í•„í„° ê°’ì„ í•¨ê»˜ ì „ì†¡
                    search_url = f"{db_server_url.rstrip('/')}/search"
                    search_payload = {
                        "company_id": selected_company_id, 
                        "query": prompt,
                        "judgment_filter": judgment_to_send
                    }
                    response = requests.post(search_url, json=search_payload, timeout=30)
                    response.raise_for_status()
                    retrieved_data = response.json().get('documents', [])

                    if not retrieved_data:
                        st.warning(f"'{judgment_filter}' ì¡°ê±´ìœ¼ë¡œ ê´€ë ¨ëœ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        with st.spinner("Geminiê°€ ê²€ìƒ‰ëœ ë¬¸í—Œì„ ë¶„ì„í•˜ì—¬ ë‹µë³€ ìƒì„± ì¤‘..."):
                            # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ Geminiì— ë‹µë³€ ìƒì„± ìš”ì²­
                            llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=gemini_api_key, temperature=0.2)
                            
                            def format_docs(docs):
                                return "\n\n".join([f"--- Source: {os.path.basename(doc['metadata'].get('source', 'N/A'))} ---\n{doc['page_content']}" for doc in docs])
                            
                            rag_chain = (
                                {"context": lambda x: format_docs(retrieved_data), "question": RunnablePassthrough()}
                                | PROMPT
                                | llm
                                | StrOutputParser()
                            )
                            
                            answer = rag_chain.invoke(prompt)
                            st.markdown(answer)
                            st.session_state.messages.append({"role": "assistant", "content": answer})

                except Exception as e:
                    st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
