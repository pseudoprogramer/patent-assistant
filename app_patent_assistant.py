    import streamlit as st
    import os
    import requests
    import json

    # LangChain ë° Gemini ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain.prompts import PromptTemplate
    from langchain.schema.runnable import RunnablePassthrough
    from langchain.schema.output_parser import StrOutputParser

    # --- 1. ì• í”Œë¦¬ì¼€ì´ì…˜ ê¸°ë³¸ ì„¤ì • ë° í”„ë¡¬í”„íŠ¸ ---
    st.set_page_config(page_title="3D DRAM íŠ¹í—ˆ ë¶„ì„ Q&A", layout="wide")

    prompt_template = """
    You are a helpful AI assistant specializing in patent analysis.
    Based on the following retrieved patent documents, answer the user's question.
    If the documents don't provide enough information, say that you cannot find a relevant answer in the provided documents.
    Provide a clear and concise answer, and always cite the source patent documents you used by their filenames (e.g., `[us20230012345a1p.txt]`).

    **Retrieved Documents:**
    {context}

    **User's Question:**
    {question}

    **Your Answer:**
    """
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    # --- 2. ì‚¬ì´ë“œë°” - ì„¤ì • ---
    with st.sidebar:
        st.header("âœ¨ AI & DB ì„œë²„ ì„¤ì •")
        gemini_api_key = st.text_input("Gemini API Key", type="password", help="[Google AI Studio](https://aistudio.google.com/app/apikey)ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”.")
        db_server_url = st.text_input("DB ê²€ìƒ‰ ì„œë²„ ì£¼ì†Œ", help="ê¸°ìˆ™ì‚¬ PCì˜ Tailscale IP ë˜ëŠ” localhostë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: http://localhost:8000)")
        
        st.markdown("---")
        st.header("ğŸ“š ë¶„ì„ ëŒ€ìƒ ì„ íƒ")
        # [ìˆ˜ì •] DB ì„ íƒ ë©”ë‰´
        db_options = {"3D DRAM íŠ¹í—ˆ": "3d_dram"}
        selected_db_name = st.selectbox("ë¶„ì„í•  DBë¥¼ ì„ íƒí•˜ì„¸ìš”.", options=db_options.keys())
        selected_db_id = db_options[selected_db_name]
        
        if st.button("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
            st.session_state.messages = []
            st.rerun()

    # --- 3. ë©”ì¸ Q&A ë¡œì§ ---
    st.title(f"âš¡ {selected_db_name} ë¶„ì„ Q&A (í•˜ì´ë¸Œë¦¬ë“œ)")

    if not gemini_api_key or not db_server_url:
        st.info("ì‚¬ì´ë“œë°”ì— Gemini API Keyì™€ DB ê²€ìƒ‰ ì„œë²„ ì£¼ì†Œë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        if "messages" not in st.session_state or st.session_state.get("current_db") != selected_db_id:
            st.session_state.messages = []
            st.session_state.current_db = selected_db_id

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input(f"{selected_db_name}ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("ë¡œì»¬ DB ì„œë²„ì— ê´€ë ¨ ë¬¸í—Œì„ ìš”ì²­í•˜ëŠ” ì¤‘... (1ë‹¨ê³„)"):
                    try:
                        search_url = f"{db_server_url.rstrip('/')}/search"
                        search_payload = {"db_id": selected_db_id, "query": prompt}
                        response = requests.post(search_url, json=search_payload, timeout=30)
                        response.raise_for_status()
                        retrieved_data = response.json().get('documents', [])

                        if not retrieved_data:
                            st.warning("ê´€ë ¨ëœ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            with st.spinner("Geminiê°€ ê²€ìƒ‰ëœ ë¬¸í—Œì„ ë¶„ì„í•˜ì—¬ ë‹µë³€ ìƒì„± ì¤‘... (2ë‹¨ê³„)"):
                                llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", google_api_key=gemini_api_key, temperature=0.2)
                                
                                def format_docs(docs):
                                    return "\n\n".join([f"--- Source: {os.path.basename(doc['metadata'].get('source', 'N/A'))} ---\n{doc['page_content']}" for doc in docs])
                                
                                rag_chain = (
                                    {"context": lambda x: format_docs(retrieved_data), "question": RunnablePassthrough()}
                                    | PROMPT
                                    | llm
                                    | StrOutputParser()
                                )
                                
                                answer = rag_chain.invoke(prompt)
                                st.markdown(answer)
                                st.session_state.messages.append({"role": "assistant", "content": answer})

                    except Exception as e:
                        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    